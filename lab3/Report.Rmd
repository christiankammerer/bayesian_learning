# Exercise 2

## b)

We are presented with the *Laplace approximation* of the posterior distribution $p(\beta|y)$. In order to obtain the *analytic posterior distribution* of $\beta$, we need to first calculate the *posterior mode* $\tilde \beta$ and the *negative Hessian* matrix $J_y^{-1}(\tilde \beta)$. This can be achieved through the *log posterior* function $\ell(\beta)$.

$$\ell(\beta) = \text{log } p(y|\beta)+\text{log }p(\beta)$$

**Setting up the (log) likelihood function:**

1.  $y_i$ is Poisson-distributed under the condition of $\beta$ with *rate parameter* $\lambda_i$

$$y_i|\beta \sim \text{Poisson}(\lambda_i)$$

2.  The *probability mass function* of a poisson distributed variable is as below; $$p(y_i) = \frac{\lambda^{y_i} e^{-\lambda}}{y_i}$$

3.  The *rate parameter* $\lambda_i$ is defined by the *covariates* $\mathbf{x}_i$ and the *coefficients* $\beta$; $$\lambda_i = \exp(\mathbf{x_i}^\top\beta)$$

4.  The l*ikelihood function* $L(\beta)$ look as follows: $$L(\beta) = p(y|\beta) = \prod^n_{i=1}\frac{\lambda_i^{y_i}e^{-\lambda_i}}{y_i!} = \prod^n_{i=1} \frac{\exp(\mathbf{x}_i^\top\beta)^{y_i}\exp(-\exp(\mathbf{x}_i^\top\beta))}{y_i!}$$

5.  This leads to the following *log-likelihood function* $\ell(\beta)$ $$\ell(\beta) = \log p(y|\beta) = \sum^n_{i=1}\mathbf{x_i}^\top\beta y_i - \exp(\mathbf{x}_i^\top\beta)-\log(y_i!)$$

**Setting up the log-prior function:**

1.  We know that the prior $p(\beta)$ is a *multivariate normal distribution* with $\mu = 0$ and $\Sigma = 100(\mathbf{X^\top X}^{-1})$

2.  The *probability density function* of a multivariate normal distribution is given as;

    $$
    p(\beta) = (2\pi)^{-k/2}\lvert\Sigma\rvert^{-1/2}\exp\left(-\frac{1}{2}(\beta-\mu)^\top\Sigma^{-1}(\beta - \mu)\right)
    $$

3.  This leads to a *log-prior* function, that looks as follows:

$$
    \log p(\beta) = - \frac{k}{2}\log(2\pi)-\frac{1}{2}\log(\lvert\Sigma\rvert)-\frac{1}{2}(\beta-\mu)^\top\Sigma^{-1}(\beta_\mu)
    $$

**Log-posterior function:**

The *log-posterior* function is the product of the log-likelihood $\ell(\beta)$ and the log-prior $\log p(\beta)$, thus

$$
\log p(\beta|y) = \ell(\beta)\log p(\beta)
$$

```{r}
library(mvtnorm)
X <- read.table("eBayNumberOfBidderData_2025.dat", header = TRUE)
y <- X$nBids
X$nBids <- NULL
x <- as.matrix(X)

# Prior parameters
p <- ncol(x)
mu <- rep(0, p)
sigma <- 100 * solve(t(x) %*% x)
log_prior <- function(sigma, mu, beta){
  k <- nrow(sigma)
  
  return(
    log(2*pi) * k/2 -
      1/2 * log(det(sigma)) -
      1/2 * t(beta - mu) %*% solve(sigma) %*% (beta - mu)
  )
}

log_likelihood <- function(x, y, beta){
  n <- nrow(x)
  return(
    sum(
      sapply((1:n), function(i) t(x[i, ]) %*% beta * y[i]
             - exp(t(x[i, ]) %*% beta) 
             - log(factorial(y[i])))
    )
  )
}

log_posterior <- function(sigma, mu, x, y, beta){
  print(sigma)
  print(mu)
  return(log_prior(sigma, mu, beta) + log_likelihood(x, y, beta))
}

# Objective to minimize (negative log-posterior)
neg_log_posterior <- function(beta) {
  -log_posterior(sigma, mu, x, y, beta)
}

# Optimization (to find posterior mode)
optres <- optim(par = rep(0, p), fn = neg_log_posterior, hessian = TRUE, method = "BFGS")

# Posterior mode
beta_mode <- optres$par

# Negative Hessian at mode (i.e. observed information)
neg_hessian <- optres$hessian

# Output results
cat("Posterior mode (beta):\n")
print(beta_mode)

cat("\nNegative Hessian at mode:\n")
print(neg_hessian)


```

### c)

```{r}
metropolis_alg <- function(n, initial_vec, sample_func, c=1, sigma, ...){
  p <- length(initial_vec)
  sample_matrix <- matrix(nrow = n, ncol = length(initial_vec))
  x_t <- rmvnorm(1, mean = rep(0, p), sigma = sigma)
  print(x_t)
  f_x_t <- log_posterior(x_t, ...)
  print(f_x_t)
  for(i in 1:n){
    proposal <- rmvnorm(1, mean = x_t, sigma = c * sigma)
    
  }
}

initial_vec<- rmvnorm(1, mean = rep(0, p), sigma = diag(p))
x_t <- metropolis_alg(n = 1000, initial_vec, log_posterior, 10, neg_hessian, mu = rep(0, p), x = x, y = y)
```
