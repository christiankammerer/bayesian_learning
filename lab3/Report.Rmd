# Exercise 2

## b)

We are presented with the *Laplace approximation* of the posterior distribution $p(\beta|y)$. In order to obtain the *analytic posterior distribution* of $\beta$, we need to first calculate the *posterior mode* $\tilde \beta$ and the *negative Hessian* matrix $J_y^{-1}(\tilde \beta)$. This can be achieved through the *log posterior* function $\ell(\beta)$.

$$\ell(\beta) = \text{log } p(y|\beta)+\text{log }p(\beta)$$

**Setting up the (log) likelihood function:**

1.  $y_i$ is Poisson-distributed under the condition of $\beta$ with *rate parameter* $\lambda_i$

$$y_i|\beta \sim \text{Poisson}(\lambda_i)$$

2.  The *probability mass function* of a poisson distributed variable is as below; $$p(y_i) = \frac{\lambda^{y_i} e^{-\lambda}}{y_i}$$

3.  The *rate parameter* $\lambda_i$ is defined by the *covariates* $\mathbf{x}_i$ and the *coefficients* $\beta$; $$\lambda_i = \exp(\mathbf{x_i}^\top\beta)$$

4.  The l*ikelihood function* $L(\beta)$ look as follows: $$L(\beta) = p(y|\beta) = \prod^n_{i=1}\frac{\lambda_i^{y_i}e^{-\lambda_i}}{y_i!} = \prod^n_{i=1} \frac{\exp(\mathbf{x}_i^\top\beta)^{y_i}\exp(-\exp(\mathbf{x}_i^\top\beta))}{y_i!}$$

5.  This leads to the following *log-likelihood function* $\ell(\beta)$ $$\ell(\beta) = \log p(y|\beta) = \sum^n_{i=1}\mathbf{x_i}^\top\beta y_i - \exp(\mathbf{x}_i^\top\beta)-\log(y_i!)$$

**Setting up the log-prior function:**

1.  We know that the prior $p(\beta)$ is a *multivariate normal distribution* with $\mu = 0$ and $\Sigma = 100(\mathbf{X^\top X}^{-1})$

2.  The *probability density function* of a multivariate normal distribution is given as;

    $$
    p(\beta) = (2\pi)^{-k/2}\lvert\Sigma\rvert^{-1/2}\exp\left(-\frac{1}{2}(\beta-\mu)^\top\Sigma^{-1}(\beta - \mu)\right)
    $$

3.  This leads to a *log-prior* function, that looks as follows:

$$
    \log p(\beta) = - \frac{k}{2}\log(2\pi)-\frac{1}{2}\log(\lvert\Sigma\rvert)-\frac{1}{2}(\beta-\mu)^\top\Sigma^{-1}(\beta_\mu)
    $$

**Log-posterior function:**

The *log-posterior* function is the product of the log-likelihood $\ell(\beta)$ and the log-prior $\log p(\beta)$, thus

$$
\log p(\beta|y) = \ell(\beta)\log p(\beta)
$$

```{r}
library(mvtnorm)
set.seed(12345)
X <- read.table("eBayNumberOfBidderData_2025.dat", header = TRUE)
y <- X$nBids
X$nBids <- NULL
x <- as.matrix(X)

# Prior parameters
p <- ncol(x)
mu <- rep(0, p)
sigma <- 100 * solve(t(x) %*% x + diag(1e-6, p)) # Ensure numeric stability

log_prior <- function(sigma, mu, beta){
  k <- nrow(sigma)
  
  return(
    log(2*pi) * k/2 -
      1/2 * log(det(sigma)) -
      1/2 * t(beta - mu) %*% solve(sigma) %*% (beta - mu)
  )
}

log_likelihood <- function(x, y, beta){
  n <- nrow(x)
  return(
    sum(
      sapply((1:n), function(i) t(x[i, ]) %*% beta * y[i]
             - exp(t(x[i, ]) %*% beta) 
             - log(factorial(y[i])))
    )
  )
}

log_posterior <- function(sigma, mu, x, y, beta){
  return(log_prior(sigma, mu, beta) + log_likelihood(x, y, beta))
}

# Objective to minimize (negative log-posterior)
neg_log_posterior <- function(beta) {
  -log_posterior(sigma, mu, x, y, beta)
}

# Optimization (to find posterior mode)
optres <- optim(par = rep(0, p), fn = neg_log_posterior, hessian = TRUE, method = "BFGS")

# Posterior mode
beta_mode <- optres$par

# Negative Hessian at mode (i.e. observed information)
neg_hessian <- optres$hessian

# Output results
cat("Posterior mode (beta):\n")
print(beta_mode)

cat("\nNegative Hessian at mode:\n")
print(neg_hessian)


```

### c)

```{r}
metropolis_alg <- function(n, initial_vec, sample_func, c=1, 
                           sigma, ...){
  p <- length(initial_vec)
  sample_matrix <- matrix(nrow = n, ncol = length(initial_vec))
  x_t <- rmvnorm(1, mean = rep(0, p), sigma = sigma)
  f_x_t <- sample_func(sigma, mu, x, y, as.vector(x_t))
  for(i in 1:n){
    proposal <- rmvnorm(1, mean = x_t, sigma = c * sigma)
    f_proposal <- sample_func(sigma, mu, x, y, as.vector(proposal))
    acceptance_prob <- f_proposal - f_x_t # since we are in log-space, we subtract instead of dividing
    if(acceptance_prob >= 1 || (runif(1) <= acceptance_prob)){
      sample_matrix[i, ] <- proposal
      x_t <- proposal
      f_x_t <- f_proposal
    } else{
      sample_matrix[i, ] <- x_t
    }
  }
  return(sample_matrix)
}

samples <- metropolis_alg(n = 100j0Â´mp00, beta_mode, log_posterior, 0.0001, neg_hessian, mu = rep(0, p), x = x, y = y)
```

```{r}
matplot(samples, type = "l", lty = 1, col = rainbow(ncol(samples)),
        main = "Trace Plots", ylab = "Parameter value", xlab = "Iteration")

```

To visualize whether the *MCMC* converges, we chose to visualize the different beta-values in a trace chart. Ideally, we would see the values bounce up and down in the initial iterations and then slowly converge to a steady range, after a period that is known as **burn-in**. This takes place in our example after around $4000$ iterations.

```{r}
x_new <- c(1,   # Intercept
           1,   # PowerSeller
           0,   # VerifyID
           1,   # Sealed
           0,   # MinBlem
           1,   # MajBlem
           0,   # LargNeg
           1.3, # LogBook
           0.7) # MinBidShare

after_burnin <- samples[4000:10000, ]

eta <- after_burnin %*% x_new
lambda <- exp(eta)
y_pred <- rpois(length(lambda), lambda)

# Predictive distribution
hist(y_pred, breaks = 30, probability = TRUE,
     main = "Posterior Predictive Distribution",
     xlab = "Number of Bidders")

# Probability of zero bidders
pr_0 <- mean(y_pred == 0)
```
